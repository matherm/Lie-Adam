{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17b11a06-7976-4b4b-b4e2-a69e84dd4d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "import torchvision.transforms.functional as TF\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.covariance import LedoitWolf, MinCovDet\n",
    "\n",
    "from torchvision.transforms import transforms\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "np.random.seed(252525)\n",
    "torch.manual_seed(252525)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from data.mvtec import *\n",
    "from hugeica import *\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "14eeeb3a-2b47-40c5-9365-b54dab25ff37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, time\n",
    "import torch.nn as nn\n",
    "from torch.distributions import TransformedDistribution, Uniform, SigmoidTransform, AffineTransform\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from ummon import *\n",
    "\n",
    "def remove_model(model=\"_\"):\n",
    "    if os.path.exists(f'./{model}.pth.tar'):\n",
    "        os.remove(f'./{model}.pth.tar')\n",
    "    if os.path.exists(f'./{model}_best_training_loss.pth.tar'):\n",
    "        os.remove(f'./{model}_best_training_loss.pth.tar')\n",
    "\n",
    "class LogCosh(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_dims=128, loc=torch.zeros(1), scale=torch.ones(1) * np.sqrt(3)/np.pi):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"loc\", loc.view(-1).clone())\n",
    "        self.register_buffer(\"scale\", scale.view(-1).clone())\n",
    "        self._initialize_distributions()\n",
    "        self.device = \"cpu\"\n",
    "\n",
    "    def to(self, device):\n",
    "        super().to(device)\n",
    "        self.device = device\n",
    "    \n",
    "    def _initialize_distributions(self):\n",
    "        base_distribution = Uniform(0, 1)\n",
    "        transforms = [SigmoidTransform().inv, AffineTransform(loc=self.loc, scale=self.scale)]\n",
    "        self._distributions = TransformedDistribution(base_distribution, transforms)\n",
    "\n",
    "    def sample(self, size=(1, 100)):\n",
    "        return self._distributions.sample(size).to(self.device)\n",
    "    \n",
    "    def log_prob(self, X):\n",
    "        X = X.to(\"cpu\")\n",
    "        return self._distributions.log_prob(X).sum(axis=1).to(self.device)\n",
    "\n",
    "class Normal(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_dims=128, loc=torch.zeros(128), scale=torch.ones(128)):\n",
    "        super().__init__()\n",
    "        if n_dims != loc.shape[0]:\n",
    "            loc, scale =torch.zeros(n_dims), torch.ones(n_dims)\n",
    "        self.loc, self.scale = loc, scale\n",
    "        self._initialize_distributions()\n",
    "        self.device = \"cpu\"\n",
    "\n",
    "    def to(self, device):\n",
    "        super().to(device)\n",
    "        self.device = device\n",
    "        self.loc = self.loc.to(device)\n",
    "        self.scale = self.scale.to(device)\n",
    "        self._distributions.loc = self._distributions.loc.to(device)\n",
    "        self._distributions.scale = self._distributions.scale.to(device)\n",
    "    \n",
    "    def _initialize_distributions(self):\n",
    "        self._distributions = torch.distributions.Normal(self.loc, self.scale)\n",
    "\n",
    "    def sample(self, size=(1, 101)):\n",
    "        return self._distributions.sample(size).to(self.device)\n",
    "    \n",
    "    def log_prob(self, X):\n",
    "        return self._distributions.log_prob(X).sum(axis=1)\n",
    "\n",
    "class Laplacian(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_dims=128, loc=torch.zeros(128), scale=torch.ones(128)):\n",
    "        super().__init__()\n",
    "        if n_dims != loc.shape[0]:\n",
    "            loc, scale  =torch.zeros(n_dims), torch.ones(n_dims)\n",
    "        self.loc, self.scale = loc, scale\n",
    "        self._initialize_distributions()\n",
    "        self.device = \"cpu\"\n",
    "\n",
    "    def to(self, device):\n",
    "        super().to(device)\n",
    "        self.device = device\n",
    "    \n",
    "    def _initialize_distributions(self):\n",
    "        self._distributions = torch.distributions.laplace.Laplace(self.loc, self.scale)\n",
    "\n",
    "    def sample(self, size=(1, 100)):\n",
    "        return self._distributions.sample(size).to(self.device)\n",
    "    \n",
    "    def log_prob(self, X):\n",
    "        X = X.to(\"cpu\")\n",
    "        return self._distributions.log_prob(X).sum(axis=1).to(self.device)\n",
    "\n",
    "deep128 = {\n",
    "             \"nets\": lambda: nn.Sequential(nn.Linear(128, 256), nn.LeakyReLU(), nn.Linear(256, 256), nn.LeakyReLU(), nn.Linear(256, 128)),\n",
    "             \"nett\": lambda: nn.Sequential(nn.Linear(128, 256), nn.LeakyReLU(), nn.Linear(256, 256), nn.LeakyReLU(), nn.Linear(256, 128))\n",
    "            }\n",
    "\n",
    "\n",
    "isa128 = {\n",
    "             \"nets\": lambda: nn.Sequential(nn.Linear(128, 128)),\n",
    "             \"nett\": lambda: nn.Sequential(nn.Linear(128, 128))\n",
    "            }\n",
    "\n",
    "\n",
    "def random_mask(layers, n_dims):\n",
    "    masks = np.zeros((layers, n_dims)).astype(np.float32)\n",
    "    for i in range(layers):\n",
    "        idx =  np.random.uniform(0, n_dims, np.max([1, n_dims//2])).astype(np.int32)\n",
    "        masks[i, idx] = 1.\n",
    "    masks = torch.from_numpy(masks)   \n",
    "    return masks\n",
    "\n",
    "def checkerboard(layers, n_dims):\n",
    "    rows = cols = int(np.sqrt(n_dims))\n",
    "    masks = np.zeros((layers, rows, cols)).astype(np.float32)\n",
    "    for i in range(layers):\n",
    "        for row in range(rows):\n",
    "            for col in range(cols):\n",
    "                masks[i, row, col] = 1. if (row % 2 == 0 and col % 2 == 0) or (row % 2 == 1 and col % 2 == 1) else 0.\n",
    "    masks = torch.from_numpy(masks).view(layers, -1)   \n",
    "    for i in range(len(masks)):\n",
    "        if np.random.uniform() > 0.5: # invert mask with probability 0.5\n",
    "            masks[i] = torch.abs(masks[i] - 1)\n",
    "    return masks\n",
    "\n",
    "def channelwise_checkerboard(layers, n_dims, fmaps=1, patch_size=32):\n",
    "    rows = cols = patch_size\n",
    "    masks = np.zeros((layers, fmaps, rows, cols)).astype(np.float32)\n",
    "    for i in range(layers):\n",
    "        for f in range(fmaps):\n",
    "            for row in range(rows):\n",
    "                for col in range(cols):\n",
    "                    masks[i, f, row, col] = 1. if (row % 2 == 0 and col % 2 == 0) or (row % 2 == 1 and col % 2 == 1) else 0.\n",
    "            if np.random.uniform() > 0.5: # invert mask with probability 0.5\n",
    "                masks[i, f] = np.abs( masks[i, f] - 1)\n",
    "    masks = torch.from_numpy(masks).view(layers, -1)   \n",
    "    return masks\n",
    "\n",
    "def channelwise_checkerboard_random(layers, n_dims, fmaps=1, patch_size=32, n=1):\n",
    "    assert n_dims == fmaps*patch_size*patch_size\n",
    "    mask = np.zeros((layers, n_dims)).astype(np.float32)\n",
    "    block = (n+n+1)*(n+n+1)\n",
    "    n_blocks = int((n_dims / 2) / (block)) # pixels of a single block\n",
    "    stepsize_channel = patch_size**2\n",
    "    for i in range(layers):\n",
    "        current_mask = np.zeros(fmaps*patch_size*patch_size).reshape(fmaps, patch_size, patch_size).astype(np.float32)\n",
    "        for b in range(n_blocks):\n",
    "            fi = int(np.random.uniform(0, fmaps))          # choose fmap, always 0 if fmaps == 1\n",
    "            xi = int(np.random.uniform(0+n, patch_size-n)) # choose xpos\n",
    "            yi = int(np.random.uniform(0+n, patch_size-n)) # choose ypos\n",
    "            current_mask[fi, yi-n:yi+n, xi-n:xi+n ] = 1.\n",
    "        mask[i] = current_mask.flatten()\n",
    "    mask = torch.from_numpy(mask)\n",
    "    return mask\n",
    "\n",
    "def channelwise(layers, n_dims, fmaps=1, patch_size=32):\n",
    "    assert fmaps > 1\n",
    "    assert n_dims == fmaps*patch_size*patch_size\n",
    "    mask = np.zeros((layers, n_dims)).astype(np.float32)\n",
    "    stepsize = patch_size**2\n",
    "    for i in range(layers):\n",
    "        idx = np.random.uniform(0, fmaps, fmaps//2).astype(np.int32)\n",
    "        for ii in idx:\n",
    "            mask[i, ii*stepsize:(ii+1)*stepsize] = 1.\n",
    "    mask = torch.from_numpy(mask)\n",
    "    return mask\n",
    "\n",
    "class Reshape(nn.Module):\n",
    "    \n",
    "    def __init__(self, shape):\n",
    "        super().__init__()\n",
    "        self.shape = shape\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return X.view(len(X), *self.shape)\n",
    "\n",
    "class BPD(OnlineMetric):\n",
    "\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def __call__(self, z, _):\n",
    "        \"\"\"\n",
    "        See Also:\n",
    "            Equation (3) in the RealNVP paper: https://arxiv.org/abs/1605.08803\n",
    "            - or -\n",
    "            Page 12 in https://arxiv.org/pdf/1705.07057.pdf\n",
    "        \"\"\"\n",
    "        n, dim = z.shape[0], z.shape[1]\n",
    "        nll = -(self.model.prior.log_prob(z) - np.log(256)*dim  + self.model.log_det_J).mean()\n",
    "        bpd = nll / (np.log(2) * dim)\n",
    "        return bpd\n",
    "\n",
    "class RealNVP(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple RealNVP\n",
    "    https://github.com/bayesgroup/deepbayes-2019/blob/master/seminars/day3/nf/nf-assignment.ipynb\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,nets = lambda: nn.Sequential(nn.Linear(128, 256), nn.LeakyReLU(), nn.Linear(256, 256), nn.LeakyReLU(), nn.Linear(256, 128)),\n",
    "                      nett = lambda: nn.Sequential(nn.Linear(128, 256), nn.LeakyReLU(), nn.Linear(256, 256), nn.LeakyReLU(), nn.Linear(256, 128)),\n",
    "                      mask = None, prior = None, distr=\"normal\", layers=3, n_dims=128, patch_size=None, fmaps=None, logits=False):\n",
    "        super().__init__()\n",
    "        assert distr in [\"logcosh\" , \"normal\", \"laplacian\"]\n",
    "        \n",
    "        if mask is None:\n",
    "            if patch_size is None:\n",
    "                mask = random_mask(layers, n_dims)\n",
    "            elif fmaps <= 3:\n",
    "                mask = channelwise_checkerboard(layers, n_dims, fmaps=fmaps, patch_size=patch_size)\n",
    "            else:\n",
    "                mask = channelwise(layers, n_dims, fmaps=fmaps, patch_size=patch_size)\n",
    "\n",
    "        if prior is None:\n",
    "            if distr == \"logcosh\":\n",
    "                prior = LogCosh(n_dims)\n",
    "            if distr == \"normal\":\n",
    "                prior = Normal(n_dims)\n",
    "            if distr == \"laplacian\":\n",
    "                prior = Laplacian(n_dims)\n",
    "\n",
    "        self.logits = logits\n",
    "        self.prior = prior\n",
    "        self.mask = nn.Parameter(mask, requires_grad=False)\n",
    "        self.s = torch.nn.ModuleList([nets() for _ in range(len(mask))])\n",
    "        self.t = torch.nn.ModuleList([nett() for _ in range(len(mask))])\n",
    "\n",
    "    def to(self, device):\n",
    "        self.prior.to(device)\n",
    "        super().to(device)\n",
    "        return self\n",
    "\n",
    "    def g(self, z):\n",
    "        x = z\n",
    "        for i in range(len(self.t)):\n",
    "            x_ = x * self.mask[i]\n",
    "            s = self.s[i](x_) * (1 - self.mask[i])\n",
    "            s = torch.tanh(s)\n",
    "            t = self.t[i](x_)*(1 - self.mask[i])\n",
    "            x = x_ + (1 - self.mask[i]) * (x * torch.exp(s) + t)\n",
    "        return x\n",
    "\n",
    "    def f(self, x):\n",
    "        log_det_J, z = x.new_zeros(x.shape[0]), x\n",
    "        for i in reversed(range(len(self.t))):\n",
    "            z_ = self.mask[i] * z # ON nodes\n",
    "            s = self.s[i](z_) * (1-self.mask[i]) # OFF nodes\n",
    "            s = torch.tanh(s)\n",
    "            t = self.t[i](z_) * (1-self.mask[i]) # OFF nodes\n",
    "            z = (1 - self.mask[i]) * (z - t) * torch.exp(-s) + z_\n",
    "            log_det_J -= s.sum(dim=1)\n",
    "        return z, log_det_J\n",
    "\n",
    "    def predict(self, X, bs=100, log_prob=False):\n",
    "        if log_prob:\n",
    "            pred = lambda x: self.log_prob(torch.from_numpy(x).to(self.prior.device)).cpu().view(-1, 1).detach().numpy()[:,0]\n",
    "        else:\n",
    "            pred = lambda x: self.f(torch.from_numpy(x).to(self.prior.device))[0].cpu().detach().numpy()\n",
    "        return np.concatenate([pred(X[i*bs:i*bs+bs]) for i in range(int(np.ceil(len(X) / bs)))], 0)\n",
    "\n",
    "    def log_prob(self,x):\n",
    "        return self.prior.log_prob(self(x)) + self.log_det_J\n",
    "\n",
    "    \n",
    "    def predict_logp(self, X, bs=100):\n",
    "        device = next(self.parameters()).device\n",
    "        zs = torch.cat([ self.forward(torch.from_numpy( X[i*bs:(i+1)*bs]).to(device)  ).detach() for i in range(0, len(X), bs)])\n",
    "        logps = self.log_prob(zs).detach().cpu().numpy()\n",
    "        return logps\n",
    "\n",
    "    def to_logit(self, x):\n",
    "        noise = torch.FloatTensor(np.random.uniform(size=x.shape)).to(x.device)\n",
    "        data_constraint = torch.FloatTensor([0.9]).to(x.device)\n",
    "        y = (x * 255. + noise) / 256.\n",
    "        y = (2 * y - 1) * data_constraint\n",
    "        y = (y + 1) / 2\n",
    "        y = torch.log(y) - torch.log(1. - y)\n",
    "\n",
    "        # Save log-determinant of Jacobian of initial transform\n",
    "        ldj = F.softplus(y) + F.softplus(-y) \\\n",
    "            - F.softplus((1. - torch.log(data_constraint) - torch.log(data_constraint)))\n",
    "        sldj = ldj.view(ldj.size(0), -1).sum(-1)\n",
    "        return y, sldj\n",
    "\n",
    "    def forward(self, x):\n",
    "        sldj = 0.\n",
    "        if self.logits:\n",
    "            x, sldj = self.to_logit(x)\n",
    "        z, log_det_J = self.f(x)\n",
    "        self.log_det_J = sldj + log_det_J\n",
    "        self.H = z\n",
    "        return z\n",
    "\n",
    "    def sample(self, batchSize, prior=None): \n",
    "        if prior is None:\n",
    "            z = self.prior.sample((batchSize, 1))\n",
    "        else:\n",
    "            z = prior.sample((batchSize, 1))            \n",
    "        logp = self.prior.log_prob(z)\n",
    "        x = self.g(z[:, 0, :])\n",
    "        return x.cpu().detach()\n",
    "\n",
    "    def fit(self, X, epochs, lr=1e-4, bs=100, log_interval=10, validation_ratio=0, validation_set=None):\n",
    "\n",
    "        if validation_ratio > 0.:\n",
    "            assert validation_set is None\n",
    "            idx = np.random.permutation(len(X))\n",
    "            self.idx = idx\n",
    "            num_train = int( X.shape[0] * (1. - validation_ratio) )\n",
    "            X_val = X[idx[num_train:]]\n",
    "            X = X[idx[:num_train]]\n",
    "        else:\n",
    "            X_val = validation_set\n",
    "\n",
    "        loss = lambda z, _ : -(self.prior.log_prob(z) + self.log_det_J).sum() / (z.shape[0] * z.shape[1])\n",
    "\n",
    "        optimizer = torch.optim.Adam([p for p in self.parameters() if p.requires_grad==True], lr=lr)\n",
    "\n",
    "        m_name = \"./__cache__/\" + str(time.time())\n",
    "        remove_model(m_name)\n",
    "\n",
    "        with Logger(loglevel=20, log_epoch_interval=log_interval) as lg:   \n",
    "            trs = Trainingstate(m_name)\n",
    "            scheduler = StepLR_earlystop(optimizer, trs, self, step_size=2000, nsteps=3, logger=lg, mode='min', gamma=0.1, patience=50)\n",
    "            tt = KamikazeTrainer(lg, self, loss, optimizer, use_cuda=torch.cuda.is_available(), scheduler=scheduler, trainingstate=trs, convergence_eps=0.0000001, combined_training_epochs=0)\n",
    "            tt.fit((X, bs), validation_set=X_val, epochs=epochs, metrics=[ BPD(self)])\n",
    "\n",
    "        print(\"loading best model..\")\n",
    "        trs.maybe_load_best_available_model_(self)\n",
    "        remove_model(m_name)\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cb1a3da9-8d42-46e4-8dd5-e9798adea778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56, 28, 28)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 13\n",
    "clazz = 13\n",
    "epochs = 1\n",
    "augment = False\n",
    "layer = 4\n",
    "\n",
    "net = models.efficientnet_b4(pretrained=True).features[:layer]\n",
    "net = net.to(device)\n",
    "net.eval()\n",
    "        \n",
    "X_, X_valid_, X_test_, X_labels_, T = zip(*[dataloader(clazz, P=224, s=224, label_per_patch=False, augment=augment) for i in range(epochs)])\n",
    "X__, X_valid_, X_test_ = np.concatenate(X_), np.concatenate(X_valid_), np.concatenate(X_test_)\n",
    "\n",
    "_, X_valid__, X_test__, X_labels_, T = dataloader(clazz, P=224, s=224, label_per_patch=False, augment=False)\n",
    "    \n",
    "with torch.no_grad():\n",
    "    X_ = torch.cat([ net(torch.from_numpy( X__[i:i+bs] ).to(device) ).detach().cpu() for i in range(0, len(X__), bs)]).cpu().numpy()\n",
    "    X_valid_ = torch.cat([ net(torch.from_numpy( X_valid__[i:i+bs] ).to(device) ).detach().cpu() for i in range(0, len(X_valid__), bs)]).cpu().numpy()\n",
    "    X_test_ = torch.cat([ net(torch.from_numpy( X_test__[i:i+bs] ).to(device) ).detach().cpu() for i in range(0, len(X_test__), bs)]).cpu().numpy()\n",
    "    \n",
    "net = net.to(\"cpu\")\n",
    "X_.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "be49d3a8-09ef-49e7-a2cb-ae707326d551",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = X_.shape[1:]\n",
    "\n",
    "ndim = np.prod(shape)\n",
    "complexity = shape[0]\n",
    "fmaps = shape[0]\n",
    "hmaps = fmaps \n",
    "p = X_.shape[2]\n",
    "\n",
    "nets = lambda: nn.Sequential(Reshape(shape), nn.Conv2d(fmaps, hmaps, 3, padding=1), nn.LeakyReLU(), nn.Conv2d(hmaps, fmaps, 3, padding=1), nn.LeakyReLU(), Reshape((ndim,)))\n",
    "nett = lambda: nn.Sequential(Reshape(shape), nn.Conv2d(fmaps, hmaps, 3, padding=1), nn.LeakyReLU(), nn.Conv2d(hmaps, fmaps, 3, padding=1), nn.LeakyReLU(), Reshape((ndim,)))\n",
    "#nett = lambda: nn.Sequential(nn.Linear(ndim, complexity), nn.LeakyReLU(), nn.Linear(complexity, ndim))\n",
    "            \n",
    "flow = RealNVP(nets, nett, n_dims=ndim, patch_size=p, fmaps=fmaps, layers=8, mask=channelwise(8, ndim, fmaps=fmaps, patch_size=p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d031e3ba-8766-4125-8da7-dfdbca9a1518",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scheduler: 3 learning rates decreased by factor 0.1 after 2000 epochs, early stopping after 50, min mode.\n",
      "Begin training: 800 epochs.\n",
      "Epoch: 10 - loss(trn/val):3.06742/3.07871, BPD(trn/val):12.43/12.44, lr=0.00010 [BEST]. [0s] @1928 samples/s \n",
      "Epoch: 20 - loss(trn/val):2.96675/2.98026, BPD(trn/val):12.28/12.30, lr=0.00010 [BEST]. [0s] @1920 samples/s \n",
      "Epoch: 30 - loss(trn/val):2.94733/2.96257, BPD(trn/val):12.25/12.27, lr=0.00010 [BEST]. [0s] @1924 samples/s \n",
      "Epoch: 40 - loss(trn/val):2.93690/2.95255, BPD(trn/val):12.24/12.26, lr=0.00010 [BEST]. [0s] @1925 samples/s \n",
      "Epoch: 50 - loss(trn/val):2.93412/2.95061, BPD(trn/val):12.23/12.26, lr=0.00010. [0s] @1924 samples/s \n",
      "Epoch: 60 - loss(trn/val):2.93091/2.94808, BPD(trn/val):12.23/12.25, lr=0.00010. [0s] @1918 samples/s \n",
      "Epoch: 70 - loss(trn/val):2.93000/2.94753, BPD(trn/val):12.23/12.25, lr=0.00010 [BEST]. [0s] @1910 samples/s \n",
      "Epoch: 80 - loss(trn/val):2.92923/2.94748, BPD(trn/val):12.23/12.25, lr=0.00010 [BEST]. [0s] @1917 samples/s \n",
      "Epoch: 90 - loss(trn/val):2.92977/2.94783, BPD(trn/val):12.23/12.25, lr=0.00010. [0s] @1915 samples/s \n",
      "Epoch: 100 - loss(trn/val):2.92866/2.94760, BPD(trn/val):12.23/12.25, lr=0.00010. [0s] @1912 samples/s \n",
      "Epoch: 110 - loss(trn/val):2.92798/2.94738, BPD(trn/val):12.22/12.25, lr=0.00010. [0s] @1907 samples/s \n",
      "Epoch: 120 - loss(trn/val):2.92989/2.95005, BPD(trn/val):12.23/12.26, lr=0.00010. [0s] @1915 samples/s \n",
      "Epoch: 130 - loss(trn/val):2.92588/2.94689, BPD(trn/val):12.22/12.25, lr=0.00010. [0s] @1916 samples/s \n",
      "Epoch: 140 - loss(trn/val):2.93169/2.95235, BPD(trn/val):12.23/12.26, lr=0.00010. [0s] @1890 samples/s \n",
      "Epoch: 150 - loss(trn/val):2.92566/2.94678, BPD(trn/val):12.22/12.25, lr=0.00010. [0s] @1907 samples/s \n",
      "No improvement since 50 epochs. Stopping early and reloading current best model.\n",
      "Current best performance for learning rate 0.00010: 2.94641.\n",
      "Epoch: 160 - loss(trn/val):2.92702/2.94611, BPD(trn/val):12.22/12.25, lr=0.00001 [BEST]. [0s] @1862 samples/s \n",
      "Epoch: 170 - loss(trn/val):2.92613/2.94591, BPD(trn/val):12.22/12.25, lr=0.00001. [0s] @1908 samples/s \n",
      "Epoch: 180 - loss(trn/val):2.92578/2.94589, BPD(trn/val):12.22/12.25, lr=0.00001. [0s] @1903 samples/s \n",
      "Epoch: 190 - loss(trn/val):2.92634/2.94588, BPD(trn/val):12.22/12.25, lr=0.00001 [BEST]. [0s] @1888 samples/s \n",
      "Epoch: 200 - loss(trn/val):2.92609/2.94587, BPD(trn/val):12.22/12.25, lr=0.00001. [0s] @1903 samples/s \n",
      "Epoch: 210 - loss(trn/val):2.92579/2.94587, BPD(trn/val):12.22/12.25, lr=0.00001. [0s] @1903 samples/s \n",
      "Epoch: 220 - loss(trn/val):2.92577/2.94585, BPD(trn/val):12.22/12.25, lr=0.00001 [BEST]. [0s] @1905 samples/s \n",
      "Epoch: 230 - loss(trn/val):2.92622/2.94585, BPD(trn/val):12.22/12.25, lr=0.00001. [0s] @1908 samples/s \n",
      "Epoch: 240 - loss(trn/val):2.92553/2.94585, BPD(trn/val):12.22/12.25, lr=0.00001. [0s] @1905 samples/s \n",
      "Epoch: 250 - loss(trn/val):2.92564/2.94587, BPD(trn/val):12.22/12.25, lr=0.00001. [0s] @1899 samples/s \n",
      "Epoch: 260 - loss(trn/val):2.92635/2.94586, BPD(trn/val):12.22/12.25, lr=0.00001. [0s] @1899 samples/s \n",
      "Epoch: 270 - loss(trn/val):2.92583/2.94588, BPD(trn/val):12.22/12.25, lr=0.00001. [0s] @1896 samples/s \n",
      "Epoch: 280 - loss(trn/val):2.92547/2.94584, BPD(trn/val):12.22/12.25, lr=0.00001. [0s] @1903 samples/s \n",
      "Epoch: 290 - loss(trn/val):2.92523/2.94587, BPD(trn/val):12.22/12.25, lr=0.00001. [0s] @1897 samples/s \n",
      "Epoch: 300 - loss(trn/val):2.92499/2.94585, BPD(trn/val):12.22/12.25, lr=0.00001. [0s] @1899 samples/s \n",
      "Epoch: 310 - loss(trn/val):2.92557/2.94581, BPD(trn/val):12.22/12.25, lr=0.00001. [0s] @1897 samples/s \n",
      "Epoch: 320 - loss(trn/val):2.92471/2.94583, BPD(trn/val):12.22/12.25, lr=0.00001. [0s] @1901 samples/s \n",
      "Epoch: 330 - loss(trn/val):2.92506/2.94582, BPD(trn/val):12.22/12.25, lr=0.00001. [0s] @1899 samples/s \n",
      "Epoch: 340 - loss(trn/val):2.92497/2.94583, BPD(trn/val):12.22/12.25, lr=0.00001. [0s] @1900 samples/s \n",
      "Epoch: 350 - loss(trn/val):2.92496/2.94582, BPD(trn/val):12.22/12.25, lr=0.00001. [0s] @1896 samples/s \n",
      "No improvement since 50 epochs. Stopping early and reloading current best model.\n",
      "Current best performance for learning rate 0.00001: 2.94581.\n",
      "Training has converged. Epsilon was 1.00e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading best model..\n"
     ]
    }
   ],
   "source": [
    "res = flow.fit(X_.reshape(len(X_), -1), epochs=800, lr=1e-4, bs=50, log_interval=10, validation_ratio=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5fa99dd9-2577-45e6-b3bd-22234ede4dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "screw 0.5187804878048781\n"
     ]
    }
   ],
   "source": [
    "scores_inliers = -flow.predict_logp(X_valid_.reshape(len(X_valid_), -1))\n",
    "scores_outliers = -flow.predict_logp(X_test_.reshape(len(X_test_), -1))\n",
    "\n",
    "auc = roc_auc_score([0] * len(scores_inliers) + [1] * len(scores_outliers), np.concatenate([scores_inliers, scores_outliers]))\n",
    "print(MVTEC.CLASSES[clazz], auc) # augmentation on + shift 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cf8292a-fff3-4246-8f72-a8a55516246d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "screw 0.8553999999999999\n"
     ]
    }
   ],
   "source": [
    "scores_inliers = -flow.predict_logp(X_valid_.reshape(len(X_valid_), -1))\n",
    "scores_outliers = -flow.predict_logp(X_test_.reshape(len(X_test_), -1))\n",
    "\n",
    "auc = roc_auc_score([0] * len(scores_inliers) + [1] * len(scores_outliers), np.concatenate([scores_inliers, scores_outliers]))\n",
    "print(MVTEC.CLASSES[clazz], auc) # augmentation on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "36927301-7c23-439f-9f8b-5f8cab2f6d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "screw 0.9009756097560975\n"
     ]
    }
   ],
   "source": [
    "scores_inliers = -flow.predict_logp(X_valid_.reshape(len(X_valid_), -1))\n",
    "scores_outliers = -flow.predict_logp(X_test_.reshape(len(X_test_), -1))\n",
    "\n",
    "auc = roc_auc_score([0] * len(scores_inliers) + [1] * len(scores_outliers), np.concatenate([scores_inliers, scores_outliers]))\n",
    "print(MVTEC.CLASSES[clazz], auc) # augmentation off"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
